usage: launch.py [-h] [--nnodes NNODES] [--nproc_per_node NPROC_PER_NODE]
                 [--rdzv_backend RDZV_BACKEND] [--rdzv_endpoint RDZV_ENDPOINT]
                 [--rdzv_id RDZV_ID] [--rdzv_conf RDZV_CONF] [--standalone]
                 [--max_restarts MAX_RESTARTS]
                 [--monitor_interval MONITOR_INTERVAL]
                 [--start_method {spawn,fork,forkserver}] [--role ROLE] [-m]
                 [--no_python] [--run_path] [--log_dir LOG_DIR] [-r REDIRECTS]
                 [-t TEE] [--node_rank NODE_RANK] [--master_addr MASTER_ADDR]
                 [--master_port MASTER_PORT] [--use_env]
                 training_script ...

Torch Distributed Elastic Training Launcher

positional arguments:
  training_script       Full path to the (single GPU) training program/script
                        to be launched in parallel, followed by all the
                        arguments for the training script.
  training_script_args

options:
  -h, --help            show this help message and exit
  --nnodes NNODES       Number of nodes, or the range of nodes in form
                        <minimum_nodes>:<maximum_nodes>.
  --nproc_per_node NPROC_PER_NODE
                        Number of workers per node; supported values: [auto,
                        cpu, gpu, int].
  --rdzv_backend RDZV_BACKEND
                        Rendezvous backend.
  --rdzv_endpoint RDZV_ENDPOINT
                        Rendezvous backend endpoint; usually in form
                        <host>:<port>.
  --rdzv_id RDZV_ID     User-defined group id.
  --rdzv_conf RDZV_CONF
                        Additional rendezvous configuration
                        (<key1>=<value1>,<key2>=<value2>,...).
  --standalone          Start a local standalone rendezvous backend that is
                        represented by a C10d TCP store on port 29400. Useful
                        when launching single-node, multi-worker job. If
                        specified --rdzv_backend, --rdzv_endpoint, --rdzv_id
                        are auto-assigned; any explicitly set values are
                        ignored.
  --max_restarts MAX_RESTARTS
                        Maximum number of worker group restarts before
                        failing.
  --monitor_interval MONITOR_INTERVAL
                        Interval, in seconds, to monitor the state of workers.
  --start_method {spawn,fork,forkserver}
                        Multiprocessing start method to use when creating
                        workers.
  --role ROLE           User-defined role for the workers.
  -m, --module          Change each process to interpret the launch script as
                        a Python module, executing with the same behavior as
                        'python -m'.
  --no_python           Skip prepending the training script with 'python' -
                        just execute it directly. Useful when the script is
                        not a Python script.
  --run_path            Run the training script with runpy.run_path in the
                        same interpreter. Script must be provided as an abs
                        path (e.g. /abs/path/script.py). Takes precedence over
                        --no_python.
  --log_dir LOG_DIR     Base directory to use for log files (e.g.
                        /var/log/torch/elastic). The same directory is re-used
                        for multiple runs (a unique job-level sub-directory is
                        created with rdzv_id as the prefix).
  -r REDIRECTS, --redirects REDIRECTS
                        Redirect std streams into a log file in the log
                        directory (e.g. [-r 3] redirects both stdout+stderr
                        for all workers, [-r 0:1,1:2] redirects stdout for
                        local rank 0 and stderr for local rank 1).
  -t TEE, --tee TEE     Tee std streams into a log file and also to console
                        (see --redirects for format).
  --node_rank NODE_RANK
                        Rank of the node for multi-node distributed training.
  --master_addr MASTER_ADDR
                        Address of the master node (rank 0). It should be
                        either the IP address or the hostname of rank 0. For
                        single node multi-proc training the --master_addr can
                        simply be 127.0.0.1; IPv6 should have the pattern
                        `[0:0:0:0:0:0:0:1]`.
  --master_port MASTER_PORT
                        Port on the master node (rank 0) to be used for
                        communication during distributed training.
  --use_env             Use environment variable to pass 'local rank'. For
                        legacy reasons, the default value is False. If set to
                        True, the script will not pass --local_rank as
                        argument, and will instead set LOCAL_RANK.
